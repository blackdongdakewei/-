# === ä¾èµ–åŒ… ===
from kokoro import KPipeline
from funasr import AutoModel
import pyaudio
import numpy as np
import requests
import json
import threading
import queue
import sounddevice as sd
import time
import cv2
import mediapipe as mp
import pickle
from screeninfo import get_monitors
from PIL import ImageGrab
from paddleocr import PaddleOCR
from PyQt5.QtWidgets import QApplication, QWidget, QLabel, QShortcut
from PyQt5.QtCore import Qt, QTimer
from PyQt5.QtGui import QPainter, QColor, QKeySequence
import sys
import keyboard

# === é…ç½®å‚æ•° ===
OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "deepseek-r1:1.5b"
AUDIO_FORMAT = pyaudio.paInt16
SAMPLE_RATE = 16000
CHUNK_SIZE = 9600
SILENT_LIMIT = 5

# === åˆå§‹åŒ–è¯­éŸ³åˆæˆ ===
tts_pipeline = KPipeline(lang_code='z')
response_queue = queue.Queue()
stop_signal = threading.Event()

# === åˆå§‹åŒ–è¯­éŸ³è¯†åˆ« ===
asr_model = AutoModel(model="paraformer-zh-streaming")
audio_interface = pyaudio.PyAudio()
input_stream = audio_interface.open(
    format=AUDIO_FORMAT,
    channels=1,
    rate=SAMPLE_RATE,
    input=True,
    frames_per_buffer=CHUNK_SIZE
)

# === TTS æ’­æ”¾çº¿ç¨‹ ===
def tts_worker():
    while True:
        text = response_queue.get()
        if text is None:
            break
        stop_signal.clear()
        generator = tts_pipeline(text, voice='af_heart', speed=1.1, split_pattern=r'[ã€‚ï¼ï¼Ÿ]')

        for _, _, audio in generator:
            if stop_signal.is_set():
                print("ğŸ”‡ æ’­æ”¾ä¸­æ–­")
                sd.stop()  # ğŸ›‘ å¼ºåˆ¶åœæ­¢å½“å‰æ’­æ”¾éŸ³é¢‘
                break
            sd.play(audio, samplerate=24000, blocking=False)
            start_time = time.time()
            while sd.get_stream().active:
                if stop_signal.is_set():
                    print("ğŸ”‡ æ’­æ”¾ä¸­æ–­")
                    sd.stop()
                    break
                time.sleep(0.05)  # é¿å…è¿‡åº¦å ç”¨CPU


# === è°ƒç”¨ LLM å›ç­” ===
def query_llm(prompt):
    try:
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªç®€æ´ä¸­æ–‡åŠ©æ‰‹ï¼š
1. å³ä½¿è¯­å¥ä¸å®Œæ•´ä¹Ÿè¦ç†è§£æ„å›¾
2. å›ç­”å£è¯­åŒ–ä½†ä¸“ä¸š
3. æœ€å¤š3å¥
4. ä¼˜å…ˆä½¿ç”¨æœ¬åœ°çŸ¥è¯†ï¼ˆæ›´æ–°è‡³2023å¹´10æœˆï¼‰
5. æŠ€æœ¯é—®é¢˜éœ€åˆ†æ­¥è¯´æ˜"""
        full_prompt = f"[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n\n{prompt} [/INST]"
        data = {
            "model": MODEL_NAME,
            "prompt": full_prompt,
            "stream": False
        }
        response = requests.post(OLLAMA_URL, json=data, timeout=30)
        if response.ok:
            result = response.json()
            response_queue.put(result['response'])
    except Exception as e:
        print(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")

# === OCR æ¨¡å‹å’Œæ³¨è§†é¢„æµ‹æ¨¡å‹ ===
ocr_engine = PaddleOCR(
    use_angle_cls=True,
    lang='ch',
    det_model_dir='C:/ocrbao/ch_PP-OCRv4_det_infer',
    rec_model_dir='C:/ocrbao/ch_PP-OCRv4_rec_infer',
    cls_model_dir='C:/ocrbao/ch_ppocr_mobile_v2.0_cls_infer'
)
screen = get_monitors()[0]
SCREEN_WIDTH, SCREEN_HEIGHT = screen.width, screen.height
with open("model.pkl", "rb") as f:
    model_x, model_y = pickle.load(f)

# === MediaPipe æ³¨è§†è·Ÿè¸ª ===
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(max_num_faces=1)
LEFT_EYE_IDs = [33, 133]
RIGHT_EYE_IDs = [362, 263]
gaze_point = [SCREEN_WIDTH // 2, SCREEN_HEIGHT // 2]

def get_eye_center(landmarks, ids, w, h):
    pts = [landmarks[i] for i in ids]
    xs = [p.x for p in pts]
    ys = [p.y for p in pts]
    return int(np.mean(xs) * w), int(np.mean(ys) * h)

def gaze_tracker():
    global gaze_point
    cap = cv2.VideoCapture(0)
    w, h = int(cap.get(3)), int(cap.get(4))
    while True:
        ret, frame = cap.read()
        if not ret:
            continue
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        result = face_mesh.process(rgb)
        if result.multi_face_landmarks:
            landmarks = result.multi_face_landmarks[0].landmark
            le = get_eye_center(landmarks, LEFT_EYE_IDs, w, h)
            re = get_eye_center(landmarks, RIGHT_EYE_IDs, w, h)
            eye_center = np.array([(le[0] + re[0]) / 2, (le[1] + re[1]) / 2]).reshape(1, -1)
            sx = int(np.clip(model_x.predict(eye_center)[0], 0, SCREEN_WIDTH))
            sy = int(np.clip(model_y.predict(eye_center)[0], 0, SCREEN_HEIGHT))
            gaze_point = [sx, sy]
    cap.release()

# === PyQt å±å¹•æ³¨è§†æ¡† ===
class GazeOverlay(QWidget):
    def __init__(self):
        super().__init__()
        self.resize(400, 180)
        self.setWindowFlags(Qt.FramelessWindowHint | Qt.WindowStaysOnTopHint | Qt.Tool)
        self.setAttribute(Qt.WA_TranslucentBackground)
        self.setFocusPolicy(Qt.StrongFocus)
        self.setFocus()
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_position)
        self.timer.start(30)
        self.smooth_x = gaze_point[0]
        self.smooth_y = gaze_point[1]
        self.label = QLabel('', self)
        self.label.move(0, 185)
        self.label.setStyleSheet("color: white; background: rgba(0,0,0,180); padding: 4px;")

        # å¿«æ·é”®ï¼šCtrl+Q è§¦å‘ OCR æé—®
        self.shortcut = QShortcut(QKeySequence("ctrl+Q"), self)
        self.shortcut.activated.connect(self.run_ocr)

        # å¿«æ·é”®ï¼šCtrl+W åœæ­¢è¯­éŸ³æ’­æŠ¥
        self.shortcut_stop = QShortcut(QKeySequence("ctrl+W"), self)
        self.shortcut_stop.activated.connect(self.stop_tts)

    def stop_tts(self):
        stop_signal.set()
        print("ğŸ›‘ å¿«æ·é”®è§¦å‘ï¼šè¯­éŸ³æ’­æŠ¥å·²ä¸­æ­¢")
        self.label.setText("è¯­éŸ³å·²åœæ­¢")

    def update_position(self):
        alpha = 0.2
        self.smooth_x = (1 - alpha) * self.smooth_x + alpha * (gaze_point[0] - self.width() // 2)
        self.smooth_y = (1 - alpha) * self.smooth_y + alpha * (gaze_point[1] - self.height() // 2)
        self.move(int(self.smooth_x), int(self.smooth_y))

    def paintEvent(self, event):
        painter = QPainter(self)
        painter.setRenderHint(QPainter.Antialiasing)
        painter.setBrush(QColor(255, 0, 0, 100))
        painter.setPen(Qt.red)
        painter.drawRect(0, 0, self.width(), self.height())

    def run_ocr(self):
        x, y, w, h = int(self.smooth_x), int(self.smooth_y), self.width(), self.height()
        img = ImageGrab.grab(bbox=(x, y, x + w, y + h))
        result = ocr_engine.ocr(np.array(img), cls=True)
        text = ' '.join([line[1][0] for line in result[0]])
        if text:
            question = f"{text} è¿™æ˜¯ä»€ä¹ˆï¼Ÿ"
            threading.Thread(target=query_llm, args=(question,), daemon=True).start()
            self.label.setText("æé—®ä¸­...")
        else:
            self.label.setText("æ— æ–‡å­—å¯è¯†åˆ«")

    def get_ocr_text(self):
        x, y, w, h = int(self.smooth_x), int(self.smooth_y), self.width(), self.height()
        img = ImageGrab.grab(bbox=(x, y, x + w, y + h))
        result = ocr_engine.ocr(np.array(img), cls=True)
        text = ' '.join([line[1][0] for line in result[0]])
        return text.strip()


# === ä¸»ç¨‹åºï¼šè¯­éŸ³è¯†åˆ«å¾ªç¯ ===
QUESTION_KEYWORDS = ['åºŸç‰©','è¿™æ˜¯',"ä»€ä¹ˆ", "å¹²å˜›", "ç”¨é€”", "ä½œç”¨", "è¿™æ˜¯", "ç”¨æ¥", "æœ‰ç”¨", "æ€ä¹ˆç”¨", "èƒ½å¹²å•¥", "åšä»€ä¹ˆ", "æœ‰å•¥", "å“ªæ–¹é¢", "æ„ä¹‰"]

def asr_loop():
    current_text = ""
    silent_count = 0
    asr_cache = {}
    print("ğŸ¤ è¯­éŸ³åŠ©æ‰‹å·²å¯åŠ¨ï¼Œç›´æ¥è®²è¯ï¼ˆCtrl+Cé€€å‡ºï¼‰...")

    while True:
        audio_data = input_stream.read(CHUNK_SIZE, exception_on_overflow=False)
        audio_array = np.frombuffer(audio_data, dtype=np.int16).astype('float32') / 32768
        result = asr_model.generate(input=audio_array, cache=asr_cache, is_final=False)

        if result and (text := result[0].get('text', '').strip()):
            current_text += text
            silent_count = 0
            print(f"\rè¯†åˆ«ä¸­: {current_text}", end='', flush=True)

            # åœæ­¢æŒ‡ä»¤
            # ğŸ›‘ æ’­æ”¾ä¸­æ–­å…³é”®è¯åˆ—è¡¨
            INTERRUPT_KEYWORDS = ["åœ", "åœä¸€ä¸‹", "åœæ­¢", "åˆ«è¯´", "ä¸è¯´äº†", "é—­å˜´", "è¡Œäº†", "å¤Ÿäº†", "æ‰“ä½", "æ‰“æ–­",
                                  "å–æ¶ˆ"]

            # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–­å…³é”®è¯
            if any(kw in current_text for kw in INTERRUPT_KEYWORDS):
                stop_signal.set()
                print("\nğŸ›‘ æ£€æµ‹åˆ°ä¸­æ–­æŒ‡ä»¤ï¼Œè¯­éŸ³æ’­æŠ¥å·²ä¸­æ­¢")
                current_text = ""
                continue

            # å¦‚æœåŒ…å«ä»»ä½•æé—®å…³é”®è¯ï¼Œå°±è§¦å‘æé—®
            if any(kw in current_text for kw in QUESTION_KEYWORDS):
                print("\nğŸ“¤ æ£€æµ‹åˆ°æé—®å…³é”®è¯ï¼Œæ‰§è¡Œ OCR + é—®ç­”")
                user_question = current_text.strip()
                ocr_text = overlay.get_ocr_text()
                if ocr_text:
                    full_prompt = f"{user_question}\n\nä»¥ä¸‹æ˜¯æˆ‘çœ‹åˆ°çš„å†…å®¹ï¼š\n{ocr_text}"
                    threading.Thread(target=query_llm, args=(full_prompt,), daemon=True).start()
                else:
                    print("âš ï¸ æ— æ³•ä»æ³¨è§†åŒºåŸŸè¯†åˆ«æ–‡å­—")
                current_text = ""
                continue

        else:
            silent_count += 1
            if silent_count >= SILENT_LIMIT and current_text:
                print("\nğŸ“¤ é™éŸ³è¶…æ—¶ï¼Œæ¸…é™¤å½“å‰è¯†åˆ«å†…å®¹")
                current_text = ""
                silent_count = 0



# === å¯åŠ¨çº¿ç¨‹ä¸ GUI ===
if __name__ == "__main__":
    threading.Thread(target=gaze_tracker, daemon=True).start()
    threading.Thread(target=tts_worker, daemon=True).start()
    app = QApplication(sys.argv)
    overlay = GazeOverlay()
    overlay.show()
    threading.Thread(target=asr_loop, daemon=True).start()
    keyboard.add_hotkey('ctrl+q', overlay.run_ocr)
    keyboard.add_hotkey('ctrl+w', overlay.stop_tts)
    sys.exit(app.exec_())
