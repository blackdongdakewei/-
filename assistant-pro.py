# === ä¾èµ–åŒ… ===
from kokoro import KPipeline
from funasr import AutoModel
import pyaudio
import numpy as np
import requests
import json
import threading
import queue
import sounddevice as sd
import time
import cv2
import mediapipe as mp
import pickle
from screeninfo import get_monitors
from PIL import ImageGrab
from paddleocr import PaddleOCR
from PyQt5.QtWidgets import QApplication, QWidget, QTextEdit, QShortcut
from PyQt5.QtCore import Qt, QTimer, pyqtSignal
from PyQt5.QtGui import QPainter, QColor, QKeySequence
import sys
import keyboard

# === é…ç½®å‚æ•° ===
OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "deepseek-r1:1.5b"
AUDIO_FORMAT = pyaudio.paInt16
SAMPLE_RATE = 16000
CHUNK_SIZE = 9600
SILENT_LIMIT = 5

# === åˆå§‹åŒ–è¯­éŸ³åˆæˆ ===
tts_pipeline = KPipeline(lang_code='z')
response_queue = queue.Queue()
stop_signal = threading.Event()

# === åˆå§‹åŒ–è¯­éŸ³è¯†åˆ« ===
asr_model = AutoModel(model="paraformer-zh-streaming")
audio_interface = pyaudio.PyAudio()
input_stream = audio_interface.open(
    format=AUDIO_FORMAT,
    channels=1,
    rate=SAMPLE_RATE,
    input=True,
    frames_per_buffer=CHUNK_SIZE
)

# === OCR å’Œæ³¨è§†é¢„æµ‹æ¨¡å‹ ===
ocr_engine = PaddleOCR(
    use_angle_cls=True,
    lang='ch',
    det_model_dir='C:/ocrbao/ch_PP-OCRv4_det_infer',
    rec_model_dir='C:/ocrbao/ch_PP-OCRv4_rec_infer',
    cls_model_dir='C:/ocrbao/ch_ppocr_mobile_v2.0_cls_infer'
)
screen = get_monitors()[0]
SCREEN_WIDTH, SCREEN_HEIGHT = screen.width, screen.height
with open("model.pkl", "rb") as f:
    model_x, model_y = pickle.load(f)

# === æ³¨è§†è·Ÿè¸ª ===
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(max_num_faces=1)
LEFT_EYE_IDs = [33, 133]
RIGHT_EYE_IDs = [362, 263]
gaze_point = [SCREEN_WIDTH // 2, SCREEN_HEIGHT // 2]

def get_eye_center(landmarks, ids, w, h):
    pts = [landmarks[i] for i in ids]
    xs = [p.x for p in pts]
    ys = [p.y for p in pts]
    return int(np.mean(xs) * w), int(np.mean(ys) * h)

def gaze_tracker():
    global gaze_point
    cap = cv2.VideoCapture(0)
    w, h = int(cap.get(3)), int(cap.get(4))
    while True:
        ret, frame = cap.read()
        if not ret:
            continue
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        result = face_mesh.process(rgb)
        if result.multi_face_landmarks:
            landmarks = result.multi_face_landmarks[0].landmark
            le = get_eye_center(landmarks, LEFT_EYE_IDs, w, h)
            re = get_eye_center(landmarks, RIGHT_EYE_IDs, w, h)
            eye_center = np.array([(le[0] + re[0]) / 2, (le[1] + re[1]) / 2]).reshape(1, -1)
            sx = int(np.clip(model_x.predict(eye_center)[0], 0, SCREEN_WIDTH))
            sy = int(np.clip(model_y.predict(eye_center)[0], 0, SCREEN_HEIGHT))
            gaze_point = [sx, sy]
    cap.release()

# === å›ç­”å±•ç¤ºçª—å£ï¼ˆæ»šåŠ¨ç‰ˆï¼‰ ===
class ResponseWindow(QWidget):
    update_text_signal = pyqtSignal(str)

    def __init__(self):
        super().__init__()
        self.resize(400, 180)
        self.move(SCREEN_WIDTH - 420, SCREEN_HEIGHT - 220)
        self.setWindowFlags(Qt.FramelessWindowHint | Qt.WindowStaysOnTopHint | Qt.Tool)
        self.setAttribute(Qt.WA_TranslucentBackground)

        self.text_edit = QTextEdit(self)
        self.text_edit.setReadOnly(True)
        self.text_edit.setStyleSheet(
            """
            QTextEdit {
                color: white;
                background: rgba(0, 0, 0, 180);
                padding: 6px;
                font-size: 16px;
                border: none;
            }
            """
        )
        self.text_edit.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
        self.text_edit.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        self.text_edit.resize(self.width(), self.height())

        self.update_text_signal.connect(self._update_text)

    def display_text(self, text):
        self.update_text_signal.emit(text)

    def _update_text(self, text):
        self.text_edit.setPlainText(text)
        self.text_edit.moveCursor(self.text_edit.textCursor().End)

# === æ³¨è§†çº¢æ¡†è¦†ç›–çª—å£ ===
class GazeOverlay(QWidget):
    def __init__(self):
        super().__init__()
        self.resize(400, 180)
        self.setWindowFlags(Qt.FramelessWindowHint | Qt.WindowStaysOnTopHint | Qt.Tool)
        self.setAttribute(Qt.WA_TranslucentBackground)
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_position)
        self.timer.start(30)
        self.smooth_x = gaze_point[0]
        self.smooth_y = gaze_point[1]
        self.shortcut = QShortcut(QKeySequence("ctrl+Q"), self)
        self.shortcut.activated.connect(self.run_ocr)
        self.shortcut_stop = QShortcut(QKeySequence("ctrl+W"), self)
        self.shortcut_stop.activated.connect(self.stop_tts)

    def stop_tts(self):
        stop_signal.set()
        print("ğŸ›‘ å¿«æ·é”®è§¦å‘ï¼šè¯­éŸ³æ’­æŠ¥å·²ä¸­æ­¢")
        response_window.display_text("ğŸ”‡ è¯­éŸ³å·²åœæ­¢")

    def update_position(self):
        alpha = 0.2
        self.smooth_x = (1 - alpha) * self.smooth_x + alpha * (gaze_point[0] - self.width() // 2)
        self.smooth_y = (1 - alpha) * self.smooth_y + alpha * (gaze_point[1] - self.height() // 2)
        self.move(int(self.smooth_x), int(self.smooth_y))

    def paintEvent(self, event):
        painter = QPainter(self)
        painter.setRenderHint(QPainter.Antialiasing)
        painter.setBrush(QColor(255, 0, 0, 40))
        painter.setPen(QColor(255, 0, 0, 100))
        painter.drawRect(0, 0, self.width(), self.height())

    def run_ocr(self):
        x, y, w, h = int(self.smooth_x), int(self.smooth_y), self.width(), self.height()
        img = ImageGrab.grab(bbox=(x, y, x + w, y + h))
        result = ocr_engine.ocr(np.array(img), cls=True)
        text = ' '.join([line[1][0] for line in result[0]])
        if text:
            question = f"{text} è¿™æ˜¯ä»€ä¹ˆï¼Ÿ"
            threading.Thread(target=query_llm, args=(question,), daemon=True).start()
            response_window.display_text("ğŸ§  æé—®ä¸­...")
        else:
            response_window.display_text("âš ï¸ æ— æ–‡å­—å¯è¯†åˆ«")

    def get_ocr_text(self):
        x, y, w, h = int(self.smooth_x), int(self.smooth_y), self.width(), self.height()
        img = ImageGrab.grab(bbox=(x, y, x + w, y + h))
        result = ocr_engine.ocr(np.array(img), cls=True)
        text = ' '.join([line[1][0] for line in result[0]])
        return text.strip()

# === LLM è°ƒç”¨ ===
def query_llm(prompt):
    try:
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªç®€æ´ä¸­æ–‡åŠ©æ‰‹ï¼š\n1. å³ä½¿è¯­å¥ä¸å®Œæ•´ä¹Ÿè¦ç†è§£æ„å›¾\n2. å›ç­”å£è¯­åŒ–ä½†ä¸“ä¸š\n3. æœ€å¤š3å¥\n4. ä¼˜å…ˆä½¿ç”¨æœ¬åœ°çŸ¥è¯†ï¼ˆæ›´æ–°è‡³2023å¹´10æœˆï¼‰\n5. æŠ€æœ¯é—®é¢˜éœ€åˆ†æ­¥è¯´æ˜"""
        full_prompt = f"[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n\n{prompt} [/INST]"
        data = {
            "model": MODEL_NAME,
            "prompt": full_prompt,
            "stream": False
        }
        response = requests.post(OLLAMA_URL, json=data, timeout=30)
        if response.ok:
            result = response.json()
            response_queue.put(result['response'])
        else:
            response_window.display_text("âŒ æ¨¡å‹å“åº”å¤±è´¥")
    except Exception as e:
        print(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")
        response_window.display_text("âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥")

# === TTS æ’­æ”¾çº¿ç¨‹ ===
def tts_worker():
    while True:
        text = response_queue.get()
        if text is None:
            break
        stop_signal.clear()
        response_window.display_text(text)
        generator = tts_pipeline(text, voice='af_heart', speed=1.1, split_pattern=r'[ã€‚ï¼ï¼Ÿ]')
        for _, _, audio in generator:
            if stop_signal.is_set():
                print("ğŸ”‡ æ’­æ”¾ä¸­æ–­")
                sd.stop()
                break
            sd.play(audio, samplerate=24000, blocking=False)
            while sd.get_stream().active:
                if stop_signal.is_set():
                    print("ğŸ”‡ æ’­æ”¾ä¸­æ–­")
                    sd.stop()
                    break
                time.sleep(0.05)

# === è¯­éŸ³è¯†åˆ«å¾ªç¯ ===
QUESTION_KEYWORDS = ['åºŸç‰©','è¿™æ˜¯',"ä»€ä¹ˆ", "å¹²å˜›", "ç”¨é€”", "ä½œç”¨", "ç”¨æ¥", "æœ‰ç”¨", "æ€ä¹ˆç”¨", "èƒ½å¹²å•¥", "åšä»€ä¹ˆ", "æœ‰å•¥", "å“ªæ–¹é¢", "æ„ä¹‰"]

def asr_loop():
    current_text = ""
    silent_count = 0
    asr_cache = {}
    print("ğŸ¤ è¯­éŸ³åŠ©æ‰‹å·²å¯åŠ¨ï¼Œç›´æ¥è®²è¯ï¼ˆCtrl+Cé€€å‡ºï¼‰...")

    while True:
        audio_data = input_stream.read(CHUNK_SIZE, exception_on_overflow=False)
        audio_array = np.frombuffer(audio_data, dtype=np.int16).astype('float32') / 32768
        result = asr_model.generate(input=audio_array, cache=asr_cache, is_final=False)

        if result and (text := result[0].get('text', '').strip()):
            current_text += text
            silent_count = 0
            print(f"\rè¯†åˆ«ä¸­: {current_text}", end='', flush=True)

            INTERRUPT_KEYWORDS = ["åœ", "åœä¸€ä¸‹", "åœæ­¢", "åˆ«è¯´", "ä¸è¯´äº†", "é—­å˜´", "è¡Œäº†", "å¤Ÿäº†", "æ‰“ä½", "æ‰“æ–­", "å–æ¶ˆ"]
            if any(kw in current_text for kw in INTERRUPT_KEYWORDS):
                stop_signal.set()
                print("\nğŸ›‘ æ£€æµ‹åˆ°ä¸­æ–­æŒ‡ä»¤ï¼Œè¯­éŸ³æ’­æŠ¥å·²ä¸­æ­¢")
                response_window.display_text("ğŸ”‡ è¯­éŸ³ä¸­æ–­")
                current_text = ""
                continue

            if any(kw in current_text for kw in QUESTION_KEYWORDS):
                print("\nğŸ“¤ æ£€æµ‹åˆ°æé—®å…³é”®è¯ï¼Œæ‰§è¡Œ OCR + é—®ç­”")
                user_question = current_text.strip()
                ocr_text = overlay.get_ocr_text()
                if ocr_text:
                    full_prompt = f"{user_question}\n\nä»¥ä¸‹æ˜¯æˆ‘çœ‹åˆ°çš„å†…å®¹ï¼š\n{ocr_text}"
                    threading.Thread(target=query_llm, args=(full_prompt,), daemon=True).start()
                    response_window.display_text("ğŸ§  æé—®ä¸­...")
                else:
                    response_window.display_text("âš ï¸ æ— æ³•è¯†åˆ«æ³¨è§†åŒºåŸŸæ–‡å­—")
                current_text = ""
                continue
        else:
            silent_count += 1
            if silent_count >= SILENT_LIMIT and current_text:
                print("\nğŸ“¤ é™éŸ³è¶…æ—¶ï¼Œæ¸…é™¤å½“å‰è¯†åˆ«å†…å®¹")
                current_text = ""
                silent_count = 0

# === ä¸»ç¨‹åºå…¥å£ ===
if __name__ == "__main__":
    threading.Thread(target=gaze_tracker, daemon=True).start()
    threading.Thread(target=tts_worker, daemon=True).start()
    app = QApplication(sys.argv)
    overlay = GazeOverlay()
    overlay.show()
    response_window = ResponseWindow()
    response_window.show()
    threading.Thread(target=asr_loop, daemon=True).start()
    keyboard.add_hotkey('ctrl+q', overlay.run_ocr)
    keyboard.add_hotkey('q', overlay.stop_tts)
    sys.exit(app.exec_())
